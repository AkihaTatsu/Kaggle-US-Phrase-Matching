{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35741ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kaggle/input\\us-patent-phrase-to-phrase-matching\\sample_submission.csv\n",
      "./kaggle/input\\us-patent-phrase-to-phrase-matching\\test.csv\n",
      "./kaggle/input\\us-patent-phrase-to-phrase-matching\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "##### for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "##### 上一行用于提交Kaggle时使用\n",
    "for dirname, _, filenames in os.walk('./kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82431e6d",
   "metadata": {},
   "source": [
    "# 正文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c2a34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36468</th>\n",
       "      <td>8e1386cbefd7f245</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden article</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>42d9e032d1cd3242</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden box</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36470</th>\n",
       "      <td>208654ccb9e14fa3</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden handle</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36471</th>\n",
       "      <td>756ec035e694722b</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden material</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36472</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden substrate</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        anchor                  target context  score\n",
       "0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n",
       "1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n",
       "2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n",
       "3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n",
       "4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n",
       "...                 ...           ...                     ...     ...    ...\n",
       "36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n",
       "36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n",
       "36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n",
       "36471  756ec035e694722b  wood article         wooden material     B44   0.75\n",
       "36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n",
       "\n",
       "[36473 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('./kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if filename == 'train.csv':\n",
    "            train_data = pd.read_csv(os.path.join(dirname, filename))\n",
    "        elif filename == 'test.csv':\n",
    "            test_data = pd.read_csv(os.path.join(dirname, filename))\n",
    "        elif filename == 'sample_submission.csv':\n",
    "            sample_submission = pd.read_csv(os.path.join(dirname, filename))\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae13306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import random\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbaa72",
   "metadata": {},
   "source": [
    "## 训练1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5958ea",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e09ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for row in train_data.index:\n",
    "    inp_example = InputExample(texts=[train_data.loc[row, 'anchor'],\n",
    "                                     train_data.loc[row, 'target']],\n",
    "                              label=float(train_data.loc[row, 'score']))\n",
    "    processed_data.append(inp_example)\n",
    "    \n",
    "# From https://blog.csdn.net/weixin_44843824/article/details/108257594\n",
    "def subset(alist, idxs):\n",
    "    '''\n",
    "    select the elements with index idx from alist\n",
    "    '''\n",
    "    sub_list = []\n",
    "    for idx in idxs:\n",
    "        sub_list.append(alist[idx])\n",
    "\n",
    "    return sub_list\n",
    "\n",
    "def split_list(alist, group_num=4, shuffle=True, retain_left=False):\n",
    "    '''\n",
    "    Divide alist into group_num number of groups.\n",
    "    shuffle: whether to randomize the order\n",
    "    retain_left: whether the residual elements become a new group\n",
    "    '''\n",
    "\n",
    "    index = list(range(len(alist)))\n",
    "\n",
    "    if shuffle: \n",
    "        random.shuffle(index) \n",
    "    \n",
    "    elem_num = len(alist) // group_num\n",
    "    sub_lists = {}\n",
    "    \n",
    "    for idx in range(group_num):\n",
    "        start, end = idx*elem_num, (idx+1)*elem_num\n",
    "        sub_lists['set' + str(idx)] = subset(alist, index[start:end])\n",
    "    \n",
    "    if retain_left and group_num * elem_num != len(index):\n",
    "        sub_lists['set' + str(idx + 1)] = subset(alist, index[end:])\n",
    "    \n",
    "    # Return dict and list of values for the selection of the required result\n",
    "    return sub_lists, list(sub_lists.values())\n",
    "\n",
    "_, divided_data = split_list(processed_data, \n",
    "                             group_num=5, \n",
    "                             shuffle=True,\n",
    "                             retain_left=False)\n",
    "\n",
    "train_processed_data = [x for l in divided_data[:-1] for x in l]\n",
    "valid_processed_data = divided_data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da704942",
   "metadata": {},
   "source": [
    "### 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb31597",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-mpnet-base-v2\"\n",
    "batch_size = 16\n",
    "epoch_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256bbc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Fit model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db7054282b1464eabf87cb92c33ef3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a05240e97ba4a5e85f1ba852895c46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a81a5981964fd7ac45c029650d0dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a36ae93753b4989ad7ca1a60b6cacf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd97c561d7747759558c1d08b381af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_model(training_data, validation_data, model_name, batch_size, epoch_num):\n",
    "    print(\"Loading model...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    train_dataloader = DataLoader(training_data,\n",
    "                                  shuffle=True,\n",
    "                                  batch_size=batch_size)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    evaluator = EmbeddingSimilarityEvaluator.from_input_examples(validation_data)\n",
    "    warmup_steps = math.ceil(len(train_dataloader) * epoch_num * 0.1) # 10% of train data for warm-up\n",
    "    \n",
    "    print(\"Fit model...\")\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "              epochs=epoch_num,\n",
    "              evaluator=evaluator,\n",
    "              warmup_steps=warmup_steps)\n",
    "    return model\n",
    "\n",
    "model = train_model(train_processed_data, valid_processed_data, model_name, batch_size, epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(model, test_phrases):\n",
    "    embeddings1 = model.encode(test_phrases.loc[:, 'anchor'], convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(test_phrases.loc[:, 'target'], convert_to_tensor=True)\n",
    "    output_scores = [util.cos_sim(embeddings1[i], embeddings2[i]).item() for i in range(len(test_phrases))]\n",
    "    \n",
    "    return output_scores\n",
    "\n",
    "output_scores = test_score(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_closest(array):\n",
    "    interval = [-float('inf'), 0.125, 0.375, 0.625, 0.875, float('inf')]\n",
    "    new_array = []\n",
    "    for x in array:\n",
    "        for i in range(5):\n",
    "            if x >= interval[i] and x < interval[i + 1]:\n",
    "                new_array.append(i * 0.25)\n",
    "    return new_array\n",
    "\n",
    "def calc_acc(arr1, arr2):\n",
    "    match = 0\n",
    "    all = len(arr1)\n",
    "    for i in range(all):\n",
    "        if arr1[i] == arr2[i]:\n",
    "            match += 1\n",
    "    return match / all\n",
    "\n",
    "calc_acc(to_closest(output_scores), train_data['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5e463",
   "metadata": {},
   "source": [
    "## 训练2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb25eef",
   "metadata": {},
   "source": [
    "### 数据预处理：按照标签分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8000cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def divide_context(train_data, row_name=\"context\"):\n",
    "    all_contexts = train_data.loc[:, row_name].drop_duplicates()\n",
    "    print(all_contexts)\n",
    "    \n",
    "    divided_data_list = []\n",
    "    for context in all_contexts:\n",
    "        new_data = train_data.loc[train_data.loc[:, row_name] == context]\n",
    "        divided_data_list.append(new_data)\n",
    "        \n",
    "    return divided_data_list\n",
    "    \n",
    "raw_data_list = divide_context(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split_by_context(divided_data_list):\n",
    "    context_data_split_pairs = {}\n",
    "    for context_data in divided_data_list:\n",
    "        context_name = context_data.loc[0, 'context']\n",
    "\n",
    "        context_processed_data = []\n",
    "        for row in train_data.index:\n",
    "            inp_example = InputExample(texts=[context_data.loc[row, 'anchor'],\n",
    "                                             context_data.loc[row, 'target']],\n",
    "                                      label=float(context_data.loc[row, 'score']))\n",
    "            context_processed_data.append(inp_example)\n",
    "            \n",
    "        _, divided_context_data = split_list(context_processed_data, \n",
    "                                     group_num=5,\n",
    "                                     shuffle=True,\n",
    "                                     retain_left=False)\n",
    "\n",
    "        train_context_processed_data = [x for l in divided_context_data[:-1] for x in l]\n",
    "        valid_context_processed_data = divided_context_data[-1]\n",
    "        context_data_split_pairs[context_pair] = [train_context_processed_data, valid_context_processed_data]\\\n",
    "        \n",
    "split_data_list = train_valid_split_by_context(raw_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_by_context(split_divided_data_list):\n",
    "    model_list = {}\n",
    "    for context in split_divided_data_list.keys():\n",
    "        print(\"Loading model for %s...\"%context)\n",
    "        model = SentenceTransformer(model_name)\n",
    "\n",
    "        train_dataloader = DataLoader(training_data,\n",
    "                                      shuffle=True,\n",
    "                                      batch_size=batch_size)\n",
    "        train_loss = losses.CosineSimilarityLoss(model)\n",
    "        evaluator = EmbeddingSimilarityEvaluator.from_input_examples(validation_data)\n",
    "        warmup_steps = math.ceil(len(train_dataloader) * epoch_num * 0.1) # 10% of train data for warm-up\n",
    "\n",
    "        print(\"Fit model...\")\n",
    "        model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                  epochs=epoch_num,\n",
    "                  evaluator=evaluator,\n",
    "                  warmup_steps=warmup_steps)\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
