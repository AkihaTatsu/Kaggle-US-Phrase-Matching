{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这些是你没有import的包\n",
    "import gc\n",
    "import pyarrow as pa\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import datasets\n",
    "import transformers\n",
    "#下面这3段与你一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('./kaggle/input/us-patent-phrase-to-phrase-matching'):\n",
    "    for filename in filenames:\n",
    "        if filename == 'train.csv':\n",
    "            raw_data = pd.read_csv(os.path.join(dirname, filename))\n",
    "        elif filename == 'test.csv':\n",
    "            test_data = pd.read_csv(os.path.join(dirname, filename))\n",
    "        elif filename == 'sample_submission.csv':\n",
    "            sample_submission = pd.read_csv(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_eval(data, train_per=0.8, train_num=None):\n",
    "    if train_num is None:\n",
    "        train_num = int(len(data) * train_per)\n",
    "        \n",
    "    raw_data_index = list(range(len(data)))\n",
    "    random.shuffle(raw_data_index)\n",
    "    train_data = raw_data.loc[raw_data_index[:train_num]]\n",
    "    train_data = train_data.sort_index(ascending=True)\n",
    "    eval_data = raw_data.loc[raw_data_index[train_num:]]\n",
    "    eval_data = eval_data.sort_index(ascending=True)\n",
    "    return train_data, eval_data\n",
    "\n",
    "train_data, eval_data = split_train_eval(raw_data, train_num=len(raw_data) - 37 * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrameToSet(data):\n",
    "    output_data = pd.DataFrame(columns=['sentence1', 'sentence2', 'context1', 'context2', 'label', 'idx'])\n",
    "    output_data['sentence1'] = list(data['anchor'])\n",
    "    output_data['sentence2'] = list(data['target'])\n",
    "    output_data['label'] = list(data['score'])\n",
    "    output_data['idx'] = list(np.arange(0,len(output_data['label'])))\n",
    "    contexts = []\n",
    "    for x in list(data['context']):\n",
    "        contexts.append([ord(x[0])-65,10*int(x[1])+int(x[2])])\n",
    "    output_data['context1'] = list(np.array(contexts)[:,0])\n",
    "    output_data['context2'] = list(np.array(contexts)[:,1])\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrameToSet_test(data):\n",
    "    output_data = pd.DataFrame(columns=['sentence1', 'sentence2', 'context1', 'context2', 'label', 'idx'])\n",
    "    output_data['sentence1'] = list(data['anchor'])\n",
    "    output_data['sentence2'] = list(data['target'])\n",
    "    output_data['label'] = list([0.0]*len(output_data['sentence2']))\n",
    "    output_data['idx'] = list(np.arange(0,len(output_data['label'])))\n",
    "    contexts = []\n",
    "    for x in list(data['context']):\n",
    "        contexts.append([ord(x[0])-65,10*int(x[1])+int(x[2])])\n",
    "    output_data['context1'] = list(np.array(contexts)[:,0])\n",
    "    output_data['context2'] = list(np.array(contexts)[:,1])\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_train, thetest_ = split_train_eval(raw_data)\n",
    "thetrain_, theeval_ = split_train_eval(the_train)\n",
    "#如果提交代码，运行下面这段(1/3)\n",
    "'''\n",
    "thetrain_, theeval_ = split_train_eval(raw_data, train_num=len(raw_data) - 37 * 4)\n",
    "thetest_ = test_data\n",
    "'''\n",
    "###设置###\n",
    "model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "#有条件用下面这行\n",
    "#需要下载\n",
    "#model_checkpoint = \"microsoft/deberta-v3-large\"\n",
    "batch_size = 8#有条件大一些\n",
    "num_epoch = 3.0#就是float型，默认为3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaea1dddb7b483483a7b8addafb440c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e048f79b52934f009f95106ca4e8d3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc64348bdf548d9927f8992b512f124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, context1, context2. If idx, sentence1, sentence2, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 23342\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8754' max='8754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8754/8754 30:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.803076</td>\n",
       "      <td>0.784219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.826902</td>\n",
       "      <td>0.809539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.838057</td>\n",
       "      <td>0.819464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-2500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, context1, context2. If idx, sentence1, sentence2, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5836\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-3500\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-4500\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-5500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, context1, context2. If idx, sentence1, sentence2, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5836\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-6500\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-7500\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8000\\special_tokens_map.json\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-8500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, context1, context2. If idx, sentence1, sentence2, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5836\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, context1, context2. If idx, sentence1, sentence2, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5836\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1642' max='730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [730/730 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, context1, context2. If idx, sentence1, sentence2, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 7295\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "#DeBERTa模型       这是不按ABCD分类的\n",
    "thetrain = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(FrameToSet(thetrain_)))\n",
    "thetest = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(FrameToSet(thetest_)))\n",
    "theeval = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(FrameToSet(theeval_)))\n",
    "thedict = {\"train\":thetrain,\"validation\":theeval,\"test\":thetest}\n",
    "dataset = datasets.dataset_dict.DatasetDict(thedict)\n",
    "#如果提交代码，运行下面这段(3/3)\n",
    "'''\n",
    "thetrain = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(FrameToSet(thetrain_)))\n",
    "thetest = datasets.arrow_dataset.Dataset_test(pa.Table.from_pandas(FrameToSet(thetest_)))#这行不一样\n",
    "theeval = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(FrameToSet(theeval_)))\n",
    "thedict = {\"train\":thetrain,\"validation\":theeval,\"test\":thetest}\n",
    "dataset = datasets.dataset_dict.DatasetDict(thedict)\n",
    "'''\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "GLUE_TASKS = [\"stsb\"]\n",
    "for task in GLUE_TASKS:\n",
    "    \n",
    "    #List of glue keys\n",
    "    task_to_keys = {\n",
    "        \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    }\n",
    "    \n",
    "    #Select task\n",
    "    #task = \"rte\"  #cola, mrpc\n",
    "    #batch_size = 8 #10 normally, 8 for qnli\n",
    "    \n",
    "    # Load dataset based on task variable\n",
    "    actual_task = task\n",
    "    #dataset = load_dataset(\"glue\", actual_task)\n",
    "    metric = load_metric('glue', actual_task)#这行需要下载东西########################################################################\n",
    "    \n",
    "    #Collect sentence keys and labels\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    \n",
    "    # Number of logits to output\n",
    "    num_labels = 1\n",
    "    \n",
    "    ###  Tokenizing Section  ####\n",
    "    \n",
    "    #Load model\n",
    "    #model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "    \n",
    "    # Create tokenizer for respective model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, truncation=True, model_max_length=512)\n",
    "    \n",
    "    def tokenizer_func(examples):\n",
    "        if sentence2_key is None:\n",
    "            return tokenizer(examples[sentence1_key], truncation=True,)\n",
    "        return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True,)\n",
    "    \n",
    "    # tokenize sentence(s)\n",
    "    encoded_dataset = dataset.map(tokenizer_func, batched=True)\n",
    "    \n",
    "    '''\n",
    "    #model_checkpoint = \"deberta-v3-small_baseline_cola/\"\n",
    "    model_checkpoint = \"deberta-v3-small_baseline_\"+actual_task+\"/\"\n",
    "    '''\n",
    "    ###  Model Section  ####\n",
    "    \n",
    "    # Create model and attach ForSequenceClassification head\n",
    "    model_deberta = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    ########################模型定义在上面这行\n",
    "    # Type of metric for given task\n",
    "    metric_name = \"pearson\"\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-finetuned-Testing-{task}\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=metric_name,\n",
    "        eval_accumulation_steps=5,\n",
    "        num_train_epochs=num_epoch\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = predictions[:]#, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "    trainer = Trainer(\n",
    "        model_deberta,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[validation_key],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "\n",
    "    \n",
    "    ### Collect Predictions  ###\n",
    "    \n",
    "    prediction_one = trainer.predict(encoded_dataset[\"test\"])\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Clear the Cache\n",
    "    del trainer\n",
    "    del args\n",
    "    del model_deberta\n",
    "    del encoded_dataset\n",
    "    del dataset\n",
    "    del thedict\n",
    "    del theeval\n",
    "    del thetest\n",
    "    del thetrain\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.880908879296934"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction_one.predictions是最终结果\n",
    "pearsonr(prediction_one.predictions,np.array(ftest[\"label\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for idx in feval.loc[feval[\"context1\"]==7]['idx']:\n",
    "    p.append(prediction_one.predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3982123 , 0.18381532, 0.656159  , ..., 0.5411603 , 0.29826367,\n",
       "       0.4922697 ], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: idx, dtype: int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftest.loc[ftest[\"context1\"]>7]['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fae362c0344df7bcdd49524fea4a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfd5065cecb4f609fdd5b562413c56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c0f77326064dbc89b8025934cde3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/1002 03:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.028035</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.718957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.033266</td>\n",
       "      <td>0.753571</td>\n",
       "      <td>0.752480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.024622</td>\n",
       "      <td>0.779433</td>\n",
       "      <td>0.779685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 693\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 693\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 693\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 829\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [104/104 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe8c388b2fa44c5a28c779cde458c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc7dd1ffc974d5594605a4600161da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef81ebf54405404a912b679f5c4bb855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4968\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1863\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1863' max='1863' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1863/1863 06:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.022165</td>\n",
       "      <td>0.863252</td>\n",
       "      <td>0.844895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.019224</td>\n",
       "      <td>0.888040</td>\n",
       "      <td>0.870222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.899850</td>\n",
       "      <td>0.883835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1292\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1292\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1292\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1595\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321dcb56ac214effa96b9191e98d3ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3568cd760d3f4033a09c5896d8a13d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d891b5746046eda78d02002438bfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3491\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1311\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1311' max='1311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1311/1311 04:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.030843</td>\n",
       "      <td>0.698468</td>\n",
       "      <td>0.709080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.777988</td>\n",
       "      <td>0.779453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.022283</td>\n",
       "      <td>0.808052</td>\n",
       "      <td>0.806736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 843\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 843\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 843\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1088\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0590083efe3485c97d65195977abfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda8f8e93c994a509e95c233ef44bebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a20fd6a17e74b529eb32d82fbd4b4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 834\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 315\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 01:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.041797</td>\n",
       "      <td>0.600742</td>\n",
       "      <td>0.579178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.035435</td>\n",
       "      <td>0.711657</td>\n",
       "      <td>0.716740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.035478</td>\n",
       "      <td>0.719123</td>\n",
       "      <td>0.714992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 203\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 203\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 203\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 244\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f1dc97a424494894ec2cfdf1571303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5a97556b0847e3b87b3425f25728d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970c9dc8d43a443aae98248905209a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 946\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 357\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [357/357 01:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.063039</td>\n",
       "      <td>0.596748</td>\n",
       "      <td>0.589063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.034660</td>\n",
       "      <td>0.739657</td>\n",
       "      <td>0.719294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.035129</td>\n",
       "      <td>0.760453</td>\n",
       "      <td>0.744818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 252\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 252\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 252\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 309\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbb8d7c531545c582fc72d4a90d045e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacfcdf88b0a46c88db6f627c9d9575d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf8d89cba184fba8f7d737e23c03f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2602\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='978' max='978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [978/978 03:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.033729</td>\n",
       "      <td>0.752451</td>\n",
       "      <td>0.734143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>0.804731</td>\n",
       "      <td>0.771408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.818745</td>\n",
       "      <td>0.782222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 622\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 622\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 622\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 798\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea93c8cbd7a046f3bea381e0472736a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae475673c571443bb6019dbaabc908b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c12700f2d84f9e9a876795def67d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3842\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1443\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1443' max='1443' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1443/1443 04:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.049247</td>\n",
       "      <td>0.616768</td>\n",
       "      <td>0.601901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>0.694062</td>\n",
       "      <td>0.680138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.036158</td>\n",
       "      <td>0.713579</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 927\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 927\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 927\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1202\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/151 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fcdcce0ae3426d96efa109313cfb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2721b6bdcafc4878ad86efd48b2f63b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dd04d96cbe437f9abd69a1467561bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.19.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Mottled-panpipe/.cache\\huggingface\\transformers\\ce3185000148731a86ceaf533caa85fe513fc79e02b7fe5831fb1ed52a0e0d22.7e73b1561275ae3b633ba76ab7e4889d28d73dbcdc008cbc2414369b39da319b\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3991\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1497\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1497' max='1497' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1497/1497 05:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.042476</td>\n",
       "      <td>0.689251</td>\n",
       "      <td>0.698220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.036553</td>\n",
       "      <td>0.767487</td>\n",
       "      <td>0.760136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>0.783792</td>\n",
       "      <td>0.774116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1004\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1004\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\n",
      "Configuration saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\config.json\n",
      "Model weights saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in microsoft/deberta-v3-small-finetuned-Testing-stsb\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1004\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2, __index_level_0__, context1, context2. If idx, sentence1, sentence2, __index_level_0__, context1, context2 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1230\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#DeBERTa模型       按ABCD分类\n",
    "ftrain = FrameToSet(thetrain_)\n",
    "ftest = FrameToSet(thetest_)\n",
    "feval = FrameToSet(theeval_)\n",
    "#如果提交代码，运行下面这段(2/3)\n",
    "'''\n",
    "ftrain = FrameToSet(thetrain_)\n",
    "ftest = FrameToSet_test(thetest_)#这行不一样\n",
    "feval = FrameToSet(theeval_)\n",
    "'''\n",
    "\n",
    "pre = []\n",
    "\n",
    "metric = load_metric('glue', \"stsb\")#这行需要下载东西##########################################################################\n",
    "#Load model\n",
    "#model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "# Create tokenizer for respective model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, truncation=True, model_max_length=512)\n",
    "    \n",
    "def tokenizer_func(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True,)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True,)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(9):\n",
    "    if i == 8:#bigger context not in A~H\n",
    "        all_id = ftest.loc[ftest[\"context1\"]>7]['idx']\n",
    "        if len(all_id) > 0:\n",
    "            p = []\n",
    "            for idx in all_id:\n",
    "                p.append(prediction_one.predictions[idx])\n",
    "            pre.append(np.vstack((np.array(p), np.array(ftest.loc[ftest[\"context1\"]>7]['idx']))))\n",
    "        break\n",
    "    thetrain = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(ftrain.loc[ftrain[\"context1\"]==i]))\n",
    "    thetest = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(ftest.loc[ftest[\"context1\"]==i]))\n",
    "    theeval = datasets.arrow_dataset.Dataset(pa.Table.from_pandas(feval.loc[feval[\"context1\"]==i]))\n",
    "    thedict = {\"train\":thetrain,\"validation\":theeval,\"test\":thetest}\n",
    "    dataset = datasets.dataset_dict.DatasetDict(thedict)\n",
    "    task = \"stsb\"\n",
    "    #List of glue keys\n",
    "    task_to_keys = {\n",
    "        \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    }\n",
    "    #Select task\n",
    "    #task = \"rte\"  #cola, mrpc\n",
    "    #batch_size = 8 #10 normally, 8 for qnli\n",
    "    \n",
    "    # Load dataset based on task variable\n",
    "    #dataset = load_dataset(\"glue\", actual_task)\n",
    "    \n",
    "    #Collect sentence keys and labels\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    \n",
    "    # Number of logits to output\n",
    "    num_labels = 1\n",
    "    \n",
    "    ###  Tokenizing Section  ####\n",
    "    # tokenize sentence(s)\n",
    "    encoded_dataset = dataset.map(tokenizer_func, batched=True)\n",
    "    \n",
    "    '''\n",
    "    #model_checkpoint = \"deberta-v3-small_baseline_cola/\"\n",
    "    model_checkpoint = \"deberta-v3-small_baseline_\"+actual_task+\"/\"\n",
    "    '''\n",
    "    ###  Model Section  ####\n",
    "    \n",
    "    # Create model and attach ForSequenceClassification head\n",
    "    model_deberta = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    ########################模型定义在上面这行\n",
    "    \n",
    "    # Type of metric for given task\n",
    "    metric_name = \"pearson\"\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-finetuned-Testing-{task}\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=metric_name,\n",
    "        eval_accumulation_steps=5,\n",
    "        num_train_epochs=num_epoch\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = predictions[:]#, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    validation_key = \"validation\"\n",
    "    trainer = Trainer(\n",
    "        model_deberta,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[validation_key],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()#训练过程\n",
    "\n",
    "\n",
    "    \n",
    "    ### Collect Predictions  ###\n",
    "    \n",
    "    predicts = trainer.predict(encoded_dataset[\"test\"])\n",
    "    pre.append(np.vstack((predicts.predictions, np.array(ftest.loc[ftest[\"context1\"]==i]['idx']))))\n",
    "\n",
    "    \n",
    "    ## 清理gpu，不然容易炸\n",
    "    del predicts\n",
    "    del trainer\n",
    "    del args\n",
    "    del model_deberta\n",
    "    del encoded_dataset\n",
    "    del dataset\n",
    "    del thedict\n",
    "    del theeval\n",
    "    del thetest\n",
    "    del thetrain\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将各模型结果按原顺序排好\n",
    "spre = pre[0]\n",
    "for i in range(1,len(pre)):\n",
    "    spre = np.concatenate((spre, pre[i]), axis=1)\n",
    "spre = spre[:,spre[1].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8705169600300973"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sper[0]是最终的预测结果\n",
    "pearsonr(spre[0],np.array(ftest[\"label\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spre[0]\n",
    "b = prediction_one.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8919751458041703\n"
     ]
    }
   ],
   "source": [
    "ap = 0.4\n",
    "print(pearsonr(ap*a+(1-ap)*b,np.array(ftest[\"label\"]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vocabulary path (my_own_vocab_file.bin) should be a directory\n"
     ]
    }
   ],
   "source": [
    "#模型保存过程，很粗糙，你可以换着弄\n",
    "output_model_file = \"my_own_model_file.bin\"\n",
    "output_config_file = \"my_own_config_file.bin\"\n",
    "output_vocab_file = \"my_own_vocab_file.bin\"\n",
    "\n",
    "model_to_save =  model_deberta.module if hasattr( model_deberta, 'module') else  model_deberta\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Bert模型示例\n",
    "config = BertConfig.from_json_file(output_config_file)\n",
    "model = BertForQuestionAnswering(config)\n",
    "state_dict = torch.load(output_model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "tokenizer = BertTokenizer(output_vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#GPT模型示例\n",
    "config = OpenAIGPTConfig.from_json_file(output_config_file)\n",
    "model = OpenAIGPTDoubleHeadsModel(config)\n",
    "state_dict = torch.load(output_model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "tokenizer = OpenAIGPTTokenizer(output_vocab_file)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
